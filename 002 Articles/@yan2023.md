---
author: 
tags: 
Docstring: 
url: 
year: 
date:
  - 22-05-2024
time: 12:59
type: Paper
modified: 2024-05-27
---
# Take-away ðŸ¥¡
- Multimodel models with integrated input might be a nice niche for a paper. 
- 
# Annotation
> ([[yan2023.pdf#page=1&selection=61,18,75,50|yan2023, p.1]])
> AI models predominantly using imaging data alone can frequently exhibit limitations in their stability, generalizability and interpretability, with potential performance biases, artifacts, and even critical errors.5,6 This becomes especially evident in imaging studies for high-risk patients. For instance, individuals undergoing lung cancer screening are often predisposed to a significant cardiovascular disease risk, inadvertently inflating the perceived effectiveness of the AI model by capitalizing on the existing high-risk status of the study population.

`We should look more into multimodal models for risk determination in patients.` 


> ([[yan2023.pdf#page=1&selection=85,7,95,10&color=yellow|yan2023, p.1]])
> integrating all clinical data including radiological images, it offers the best opportunity to develop a comprehensive and multi-faceted risk prediction model that more accurately and reliably reflects the intricate nature of disease risks. It is essential to recognize that the exclusive use of imaging data may not entirely capture the breadth of risk factors. Integrating a range of data types can reduce reliance on a single modality, and hence, provide a more holistic, comprehensive, and unbiased insight into the patient's health condition.


> [!PDF|red] [[yan2023.pdf#page=2&selection=4,39,16,50&color=red|yan2023, p.2]]
> >  [[@huang2020.pdf]] compared various multimodal fusion model architectures capable of using pixel data from volumetric CT pulmonary angiograms alongside clinical patient data from EMRs. These models were designed to automatically detect the presence of pulmonary embolism. Notably, the best-performing multimodality model was a late fusion model, which achieved an AUROC of 0.947 [95% CI: 0.946â€“0.948] on the held-out test set, surpassing the performance of models that used either imaging or EMR data alone.

GPT: [[@huang2020.pdf]] evaluated multimodal fusion models combining CT pulmonary angiogram pixel data and EMR clinical data for pulmonary embolism detection, finding that a late fusion model achieved the highest AUROC of 0.947, outperforming single-modality models.

Noteworthy: `I would like to know if we could use these types of models for better super-resolution models. Lets assume we have more `


> [!PDF|yellow] [[yan2023.pdf#page=2&selection=30,55,35,16&color=yellow|yan2023, p.2]]
> > For instance, while structured EMR data might indicate the exact diagnoses and treatment, but more often than not, the crucial predictors of disease risk and response such as patient history, risk factors, disease trajectory, symptomatology, and the nuances of treatment response lie within the unstructured, free-text data. 

Unstructured data should be taken into account as well, and LLMs should be able to extract the relevant pieces of information from the unstructured data heap. 

`One thought I am having here is: Does there exist a risk, that we have models converge on the most prevalent diseases, because we have the most data on these types of illnesses. And on the other hand can we build a model that is more akin to Dr. House? In the sense that we want to find rare diseases as well. What would that mean mathematically? What would we need to pay attention to?`

