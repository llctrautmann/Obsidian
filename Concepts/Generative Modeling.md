---
title: Generative Modeling
date: 06-02-2024
time: 14:30
author: Luca Trautmann
tags: 
series: GDL
chapter: 1
modified: 2024-02-09
---
# Generative Modeling
## What Is Generative Modeling

> Generative modeling is a branch of machine learning that involves training a model to produce new data that is similar to a given dataset.

In generative modelling we build a model that learns from data the underlying structure that generated the samples / __observations__ in the __training data__. Once the training is completed, we use the model to generate new samples that follow the underlying structure but are not identical to the existing samples used for training. 

Each observation consists of many __features__. For an image generation problem, the features are usually the individual pixel values; for a text generation problem, the features could be individual words or groups of letters. It is our goal to build a model that can generate new sets of features that look as if they have been created using the same rules as the original data (the rules are the generative process)

Generative Models need to be probabilistic, otherwise it would not be possible to implement the randomness necessary to construct new data (images, text, etc.)

> A generative model model must include a random component that influences the individual samples generated by the model. (Foster and Friston, 2023, p. 5)

## Generative vs Discriminative Modeling

- Discriminative modeling estimates $p(y \mid \mathbf{x})$.

That is, discriminative modeling aims to model the probability of a label $y$ given some observation $\mathbf{x}$.

- Generative modeling estimates $p(\mathbf{x})$.

That is, generative modeling aims to model the probability of observing an observation $\mathbf{x}$. Sampling from this distribution allows us to generate new observations.

## The Generative Modeling Framework
- We have a dataset of observations $\mathbf{X}$.
- We assume that the observations have been generated according to some unknown distribution, $p_{\text {data }}$.
- We want to build a generative model $p_{\text {model }}$ that mimics $p_{\text {data }}$. If we achieve this goal, we can sample from $p_{\text {model }}$ to generate observations that appear to have been drawn from $p_{\text {data }}$.
- Therefore, the desirable properties of $p_{\text {model }}$ are:

1) __Accuracy__: If $p_{\text {model }}$ is high for a generated observation, it should look like it has been drawn from $p_{\text {data }}$. If $p_\text{model}$ is low for a generated observation, it should not look like it has been drawn from $p_{\text {data }}$.
2) __Generation__: It should be possible to easily sample a new observation from $p_{\text {model }}$.
4) __Representation__: It should be possible to understand how different high-level features in the data are represented by $p_{\text {model }}$.

## Representation Learning
The idea behind Representation Learning (RL) is the [[Manifold Hypothesis]]. Fundamentally, we assume that it is possible to describe a high-dimensional sample with lower-dimensional but latent variables. 

For example the image of a face does not need to be learned pixel-wise (high-dimensional) but could be described by 10 lower dimensional structures like colour, shape, light or orientation. Assuming RL, we can design a function that maps between low-dimensional and high-dimensional representations and circumvent extremely tedious work in HD space. 

A good example: 
> Figure 1-7. The biscuit tin dataset (Foster and Friston, 2023, p. 13)


Mathematically speaking, encoder-decoder techniques try to transform the highly nonlinear manifold on which the data lies (e.g., in pixel space) into a simpler latent space that can be sampled from, so that it is likely that any point in the latent space is the representation of a well-formed image. 


## Core Probability Theory
There are five core concepts that are necessary to understand for the purposes of this book:

### Sample Space:
All possible value that a random variable can take. 

### Probability Density Function:
While there is only one true density function $p_{\text {data }}(\mathbf{x})$ that is assumed to have generated the observable dataset, there are infinitely many density functions $p_{\text {model }}(\mathrm{x})$ that we can use to estimate $p_{\text {data }}(\mathbf{x})$. see [[Probability Density Functions Definition]]
### Parametric modeling:
Parametric modeling is a technique that we can use to structure our approach to finding a suitable $p_{\text {model }}(\mathbf{x})$. A parametric model is a family of density functions $p_\theta(\mathbf{x})$ that can be described using a finite number of parameters, $\theta$.

### Likelihood: 
The likelihood function measures the likelihood of the parameters $\theta$ given some data. It is defined as: $\mathscr{L}(\theta \mid \mathbf{X})=\prod_{\mathbf{x} \in \mathbf{X}} p_\theta(\mathbf{x})$ and usually combined with a monotonically increasing function to ease the computation, leading to the log likelihood: $\ell(\theta \mid \mathbf{X})=\sum_{\mathbf{x} \in \mathbf{X}} \log p_\theta(\mathbf{x})$. Note that the likelihood is a function of the parameters, not the data. It should not be interpreted as the probability that a given parameter set is correct—in other words, it is not a probability distribution over the parameter space (i.e., it doesn’t sum/integrate to 1, with respect to the parameters).

### Maximum Likelihood estimate: 
Maximum likelihood estimation is the technique that allows us to estimate $\theta$-the set of parameters $\theta$ of a density function $p_\theta(\mathbf{x})$ that is most likely to explain some observed data X. More formally:
$$
\hat{\theta}=\underset{\mathbf{x}}{\arg \max \ell}(\theta \mid \mathbf{X})
$$
$\hat{\theta}$ is also called the maximum likelihood estimate (MLE). In the world map example, the MLE is the smallest rectangle that still contains all of the points in the training set.
 
## Generative Model Taxonomy
The taxonomy for generative model can be broadly categorised into explicitly models PDFs and implicitly modelled PDFs. 

![[GDL_Taxonomy.png]]

Implicit density models:
do not aim to estimate the probability density at all, but instead focus solely on producing a stochastic process that directly generates data. The best-known example of an implicit generative model is a generative adversarial network. 

Explicit density models:
There are two types of explicit density models. Tractable density models try to build a complex density function from simple, calculable components. For example [[autoregressive models]]. 

Approximate density models include variational autoencoders, which introduce a latent variable and optimize an approximation of the joint density function. [[Energy-based models]] also utilize approximate methods, but do so via Markov chain sampling, rather than variational methods. [[Diffusion Models]] approximate the density function by training a model to gradually denoise a given image that has been previously corrupted.